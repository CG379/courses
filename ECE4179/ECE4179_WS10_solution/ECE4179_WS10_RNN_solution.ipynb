{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://www.dropbox.com/s/vold2f3fm57qp7g/ECE4179_5179_6179_banner.png?dl=1\" alt=\"ECE4179/5179/6179 Banner\" style=\"max-width: 60%;\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "</div>\n",
    "\n",
    "Welcome to week 10 of ECE4179/5179/6179! \n",
    "\n",
    "*Dun dun dun dun-da-da dun-da-da, dun-da-da!* ðŸŽ¶\n",
    "\n",
    "In an island far, far away you find yourself on a distant jungle, with nothing but your trusty laptop, equipped with the latest release of PyTorch and other beloved Python packages. \n",
    "\n",
    "What would you do? Of course you start coding and learning about **Recurrent Neural Networks (RNNs)**!\n",
    "Letâ€™s get started, May the gradients be with you! âœ¨\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"data/Mysterious-Island.jpg\" alt=\"mysterious_island\" style=\"max-width: 60%;\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "RND_SEED = 42\n",
    "np.random.seed(RND_SEED)\n",
    "random.seed(RND_SEED)\n",
    "torch.manual_seed(RND_SEED)\n",
    "\n",
    "\n",
    "# Check if CUDA is available (for NVIDIA GPUs)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA GPU detected. Using CUDA.\")\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available (for Apple Silicon M1/M2)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Apple Silicon GPU detected. Using MPS.\")\n",
    "\n",
    "# Otherwise, default to CPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #1. So you claim to be a Jedi in neural networks?</span>\n",
    "\n",
    "Over the past 10 weeks, you've achieved amazing things with neural networks. Now, I challenge you to design an MLP for a simple task: adding two 3-digit numbers. For example, given 4179 and 5179, your MLP should return their sum, 9358. To simplify things further, if the sum exceeds 9999, just return the last 4 digits (e.g., 899 + 101 = 000).\n",
    "\n",
    "Consider the input to your model as 2x3 symbols (digits), where each symbol can take 10 possible values (0 to 9), and the output will be 3 symbols. Discuss how you would approach this as a supervised learning problem with an MLP. Think about how to design the output, handle classification (if needed), process the input, and other aspects of your solution.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #2. Have a go at your MLP!</span>\n",
    "\n",
    "Great! Hopefully, you have a solid plan for Task #1. Now, let's implement your MLP in PyTorch. Below, Iâ€™ve provided a couple of functions to help prepare the data and evaluate the performance of your model. Your task is to design the model and the training loop. \n",
    "\n",
    "If you're unsure how to approach this problem, here's a suggestion to get started:\n",
    "\n",
    "The input to your MLP should be an 8D vectorâ€”where the first 4 elements represent the digits of the first number, and the last 4 represent the second number. Instead of formulating this as a regression problem (which would be inefficient since the output space isn't continuous), treat it as a classification problem. Each digit is a class, meaning you'll need 4x10 output units. Use the cross-entropy loss function, applying it separately to each 10D output vector to predict each digit. PyTorchâ€™s `CrossEntropyLoss` handles this directly.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitAdditionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to generate pairs of random n-digit numbers and their sums, \n",
    "    where the sum is truncated or padded to match the length of the input sequences.\n",
    "\n",
    "    Args:\n",
    "        size (int): The total number of samples in the dataset. Default is 1000.\n",
    "        seq_len (int): Length of the input sequences (i.e., number of digits in each number). Default is 4.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of three elements:\n",
    "            - seq1 (torch.Tensor): The first number represented as a sequence of digits.\n",
    "            - seq2 (torch.Tensor): The second number represented as a sequence of digits.\n",
    "            - sum_seq (torch.Tensor): The sum of the two numbers, truncated/padded to seq_len.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=1000, seq_len=3):\n",
    "        self.seq_len = seq_len  # Length of the sequence (number of digits in the numbers)\n",
    "        self.size = size  # Total number of data samples to generate\n",
    "        self.data = []  # Initialize an empty list to store the generated data\n",
    "        \n",
    "        # Define the range for random numbers based on the sequence length\n",
    "        min_val = 10 ** (seq_len - 1)  # Minimum value for a number with seq_len digits (e.g., 100 for 3 digits)\n",
    "        max_val = (10 ** seq_len) - 1  # Maximum value for a number with seq_len digits (e.g., 999 for 3 digits)\n",
    "        \n",
    "        # Generate 'size' number of data samples\n",
    "        for _ in range(size):\n",
    "            num1 = random.randint(min_val, max_val)  # Randomly generate the first number\n",
    "            num2 = random.randint(min_val, max_val)  # Randomly generate the second number\n",
    "            sum_result = num1 + num2  # Calculate the sum of the two numbers\n",
    "            \n",
    "            # Convert the numbers into lists of digits\n",
    "            seq1 = [int(digit) for digit in str(num1)]  # Convert num1 to a list of digits\n",
    "            seq2 = [int(digit) for digit in str(num2)]  # Convert num2 to a list of digits\n",
    "            sum_seq = [int(digit) for digit in str(sum_result)]  # Convert sum_result to a list of digits\n",
    "            \n",
    "            # Ensure the sequences are all the same length (seq_len) by truncating or padding the sum\n",
    "            if len(sum_seq) > seq_len:\n",
    "                sum_seq = sum_seq[-seq_len:]  # Truncate the sum if it has more digits than seq_len\n",
    "            else:\n",
    "                sum_seq = [0] * (seq_len - len(sum_seq)) + sum_seq  # Pad the sum with leading zeros if it has fewer digits\n",
    "            \n",
    "            # Append the generated sequence (input1, input2, and the sum) to the data list\n",
    "            self.data.append((seq1, seq2, sum_seq))\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the input-output pair (two input sequences and their sum) for a given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            A tuple of torch.Tensors:\n",
    "                - seq1: The first number represented as a sequence of digits.\n",
    "                - seq2: The second number represented as a sequence of digits.\n",
    "                - sum_seq: The sum of the two numbers, represented as a sequence of digits.\n",
    "        \"\"\"\n",
    "        seq1, seq2, sum_seq = self.data[idx]\n",
    "        return torch.tensor(seq1), torch.tensor(seq2), torch.tensor(sum_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train_size = 500  # Number of training examples\n",
    "train_dataset = DigitAdditionDataset(size=train_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_size = 1000  # Number of test examples\n",
    "test_dataset = DigitAdditionDataset(size=test_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1: [9, 8, 2]\n",
      "Sequence 2: [5, 8, 6]\n",
      "Sum       : [5, 6, 8]\n",
      "------------------------------\n",
      "Sequence 1: [3, 8, 4]\n",
      "Sequence 2: [2, 5, 9]\n",
      "Sum       : [6, 4, 3]\n",
      "------------------------------\n",
      "Sequence 1: [1, 2, 5]\n",
      "Sequence 2: [2, 8, 3]\n",
      "Sum       : [4, 0, 8]\n",
      "------------------------------\n",
      "Sequence 1: [4, 8, 8]\n",
      "Sequence 2: [7, 9, 3]\n",
      "Sum       : [2, 8, 1]\n",
      "------------------------------\n",
      "Sequence 1: [1, 0, 0]\n",
      "Sequence 2: [7, 1, 3]\n",
      "Sum       : [8, 1, 3]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch from train_loader\n",
    "seq1, seq2, sum_seq = next(iter(train_loader))\n",
    "\n",
    "# Print the first 5 data samples from the batch\n",
    "for i in range(5):\n",
    "    print(f\"Sequence 1: {seq1[i].tolist()}\")\n",
    "    print(f\"Sequence 2: {seq2[i].tolist()}\")\n",
    "    print(f\"Sum       : {sum_seq[i].tolist()}\")\n",
    "    print(\"-\" * 30)  # Separator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_addition(model, train_loader, num_epochs=10, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()  # Cross-entropy for classification\n",
    "\n",
    "    # Initialize progress bar across epochs\n",
    "    progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0, leave=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for seq1, seq2, sum_seq in train_loader:\n",
    "            seq1, seq2, sum_seq = seq1.float(), seq2.float(), sum_seq.long()  # Input in float, target in long (integer)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(seq1, seq2)  # Shape: seq1:[batch_size, seq_len] seq2:[batch_size, seq_len]\n",
    "            \n",
    "            loss = criterion(outputs, sum_seq)  # Compare predicted digit classes with actual sum digit classes\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Update the tqdm progress bar\n",
    "        progress_bar.set_postfix(loss=total_loss / len(train_loader))  # Update the loss on the progress bar\n",
    "        progress_bar.update(1)  # Update the bar by 1 epoch\n",
    "\n",
    "    # Close the progress bar when training is done\n",
    "    progress_bar.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the MLP classification model on the test dataset\n",
    "def evaluate_addition(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the MLP addition model on the test dataset by calculating\n",
    "    the number of correctly summed sequences. The function returns the accuracy as a percentage.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained MLP addition model.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader providing the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy as a percentage of correctly summed sequences.\n",
    "    \n",
    "    This function:\n",
    "    - Sets the model to evaluation mode.\n",
    "    - Iterates over the test dataset.\n",
    "    - For each batch, it predicts the digit sequences using the model.\n",
    "    - Compares the predicted sequences to the target sequences to determine how many sums are correct.\n",
    "    - Prints the number of correctly summed sequences and the accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_sum = 0  # Counter for correct sum predictions\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for seq1, seq2, sum_seq in test_loader:\n",
    "            seq1, seq2, sum_seq = seq1.float(), seq2.float(), sum_seq.long()  # Inputs as float, targets as long\n",
    "\n",
    "            outputs = model(seq1, seq2)  # Get model predictions, shape: [batch_size, 10, seq_len]\n",
    "            \n",
    "            # Get the predicted classes (digit with highest probability)\n",
    "            predicted_seq = outputs.argmax(dim=1)  # Shape: [batch_size, seq_len]\n",
    "            \n",
    "            # Count how many times the entire predicted sequence matches the target sequence\n",
    "            correct_sum += (predicted_seq == sum_seq).all(dim=1).sum().item()  # Check if all digits match\n",
    "            total_samples += seq1.size(0)  # Total number of sequences in the batch\n",
    "    \n",
    "    # Calculate accuracy as the percentage of correctly predicted sums\n",
    "    accuracy = correct_sum / total_samples * 100\n",
    "    print(f\"Number of Correctly Summed Sequences: {correct_sum}/{total_samples}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAddition(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Multilayer Perceptron (MLP) model for adding two sequences of digits.\n",
    "    The model treats the problem as a classification task, where each digit position\n",
    "    corresponds to one of 10 classes (0-9).\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): The length of the input sequences (i.e., number of digits in each sequence).\n",
    "        hidden_dim (int): The number of hidden units in the fully connected layers.\n",
    "\n",
    "    Model Structure:\n",
    "        - Input: Two sequences, each of length `seq_len`.\n",
    "        - Concatenate the sequences to form an input vector of size `2 * seq_len`.\n",
    "        - Two hidden layers with ReLU activation.\n",
    "        - Output layer: `seq_len * 10` units, reshaped to predict a 10-class digit for each position.\n",
    "\n",
    "    Input:\n",
    "        - seq1 (torch.Tensor): The first input sequence (batch_size, seq_len).\n",
    "        - seq2 (torch.Tensor): The second input sequence (batch_size, seq_len).\n",
    "\n",
    "    Output:\n",
    "        - Output (torch.Tensor): A tensor of shape (batch_size, seq_len, 10), where the last dimension\n",
    "                                 represents the 10-class prediction (0-9) for each digit position.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len  # Store seq_len for use in the forward method\n",
    "        self.fc1 = nn.Linear(seq_len * 2, hidden_dim)  # Concatenate seq1 and seq2\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, seq_len * 10)  # Output for each digit position (10 classes per position)\n",
    "        \n",
    "    def forward(self, seq1, seq2):\n",
    "        \"\"\"\n",
    "        Forward pass of the MLPAddition model.\n",
    "\n",
    "        Args:\n",
    "            seq1 (torch.Tensor): The first input sequence (batch_size, seq_len).\n",
    "            seq2 (torch.Tensor): The second input sequence (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, 10), where each 10-class vector \n",
    "                          represents the predicted digit for each position.\n",
    "        \"\"\"\n",
    "        # Concatenate the sequences along the feature dimension\n",
    "        x = torch.cat((seq1, seq2), dim=1)  # Shape: [batch_size, seq_len * 2]\n",
    "        x = torch.relu(self.fc1(x))         # First hidden layer with ReLU activation\n",
    "        x = torch.relu(self.fc2(x))         # Second hidden layer with ReLU activation\n",
    "        x = self.fc3(x)                     # Output layer, raw logits for all positions\n",
    "        x = x.view(-1, 10, self.seq_len)    # Reshape to [batch_size, 10, seq_len] for CrossEntropyLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [03:43<00:00,  8.95it/s, loss=2.03]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Summed Sequences: 249/500, Accuracy: 49.80%\n",
      "Number of Correctly Summed Sequences: 32/1000, Accuracy: 3.20%\n"
     ]
    }
   ],
   "source": [
    "mlp_addition_model = MLPAddition(seq_len=3, hidden_dim=128)\n",
    "\n",
    "# Assuming train_loader is already created with the DigitAdditionDataset\n",
    "train_addition(mlp_addition_model, train_loader, num_epochs=2000, lr=0.01)\n",
    "\n",
    "# Evaluate the MLP model on the test dataset\n",
    "acc_trn_mlp_addition = evaluate_addition(mlp_addition_model, train_loader)\n",
    "acc_tst_mlp_addition = evaluate_addition(mlp_addition_model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed for sequence data. Unlike standard feed-forward neural networks, RNNs have **recursion** in their architecture, which allows them to maintain a form of memory, known as **hidden state** across different time steps. Can you think why they might suit our addition problem?\n",
    "\n",
    "\n",
    "\n",
    "In a vanilla RNN, the recursion can be mathematically represented as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{h}^{\\{t\\}} = f\\left(\\boldsymbol{W}_{hh} \\cdot \\boldsymbol{h}^{\\{t-1\\}}  + \\boldsymbol{W}_{hx} \\cdot \\boldsymbol{x}^{\\{t\\}}  + \\boldsymbol{b}_h \\right)\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{h}^{\\{t\\}}$ is the hidden state at time step $t$.\n",
    "- $\\boldsymbol{h}^{\\{t-1\\}}$ is the hidden state from the previous time step.\n",
    "- $\\boldsymbol{x}^{\\{t\\}}$ is the input at time step $t$.\n",
    "- $\\boldsymbol{W}_{hh}$ and $\\boldsymbol{W}_{hx}$ are weights of the model, and $\\boldsymbol{b}_h$ is the bias term.\n",
    "- $f$ is the activation function (commonly $ \\tanh $ or $ \\text{ReLU} $).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"data/ece4179_ws10_rnn1.png\" alt=\"RNN Example1\" style=\"width: 50%;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To understand how an RNN process sequences, you can think of it as unrolling the recursion over time. The hidden state at each time step is updated based on the input at that time step and the hidden state from the previous time step. This allows the RNN to maintain a form of memory across different time steps.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"data/ece4179_ws10_rnn2.png\" alt=\"RNN Example2\" style=\"width: 50%;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM in PyTorch\n",
    "\n",
    "\n",
    "\n",
    "A **Long Short-Term Memory (LSTM)** network is a type of recurrent neural network (RNN) designed to handle sequence data and learn long-term dependencies. Unlike a vanilla RNN, an LSTM contains **cell states** and **gates** (e.g., forget) to control the flow of information and prevent issues like vanishing gradients.\n",
    "\n",
    "In PyTorch, an LSTM layer processes an input sequence and returns two outputs: \n",
    "1. The output for each time step.\n",
    "2. The hidden and cell states.\n",
    "\n",
    "The input to the LSTM layer in PyTorch has the following shape:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{input} = (\\text{B}, \\tau, \\text{input\\_size})\n",
    "\\end{align*}\n",
    "\n",
    "- $\\text{B}$ is the **batch size**, the number of sequences processed simultaneously.\n",
    "- $\\tau$ is the **sequence length**, which is the number of time steps in each sequence.\n",
    "- $\\text{input\\_size}$ refers to the size (number of features) of each element in the sequence.\n",
    "\n",
    "For example, in a problem where each element in the sequence is a 2D vector (two features), and we process a batch of 4 sequences of length 5 in one batch, the input will have shape `(4, 5, 2)`.\n",
    "\n",
    "LSTMs maintain a **hidden state** and a **cell state** for each layer. The hidden and cell states have the following shape:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{hidden}, \\text{cell} = (\\text{num\\_layers}, \\text{B}, \\text{hidden\\_size})\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "- $\\text{num\\_layers}$ is the number of LSTM layers.\n",
    "- $ \\text{hidden\\_size} $ is the size of the hidden state (number of features in the hidden state).\n",
    "\n",
    "If your LSTM has 2 layers, a batch size of 4, and a hidden size of 128, the hidden and cell states will have shape `(2, 4, 128)`.\n",
    "\n",
    "\n",
    "\n",
    "The output of the LSTM consists of:\n",
    "1. **Output for each time step**: This is the hidden state for each time step in the sequence.\n",
    "   \\begin{align*}\n",
    "        \\text{output} = (\\text{B}, \\tau, \\text{hidden\\_size})\\;.\n",
    "   \\]\n",
    "   \n",
    "   If you process 4 sequences of length 5, and the hidden size is 128, the output will have the shape `(4, 5, 128)`.\n",
    "\n",
    "2. **Final hidden and cell states**:\n",
    "   \\begin{align*}\n",
    "   \\text{hidden}, \\text{cell} = (\\text{num\\_layers}, \\text{B}, \\text{hidden\\_size})\n",
    "   \\end{align*}\n",
    "   These tensors contain the final hidden and cell states for each layer and for each sequence in the batch. If the LSTM has 2 layers, a batch size of 4, and a hidden size of 128, both the hidden and cell states will have shape `(2, 4, 128)`.\n",
    "\n",
    "\n",
    "### Did You Know? \n",
    "\n",
    "Very recently, advancements in LSTM architecture have led to the development of **xLSTM: Extended Long Short-Term Memory**. This new variation introduces modifications to the standard LSTM, enhancing its ability to **retain memory** over longer sequences and improve its ability to learn **long-range dependencies**. \n",
    "\n",
    "While weâ€™ll be working with the traditional LSTM in this notebook, it's exciting to know that LSTMs are continuously evolving, and innovations like xLSTM are pushing the boundaries of what these models can achieve. \n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"data/xlstm.jpg\" alt=\"xLSTM\" style=\"width: 50%;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAddition(nn.Module):\n",
    "    def __init__(self, hidden_size=128, seq_len=3, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers  # Number of LSTM layers\n",
    "        \n",
    "        # Define the LSTM layer with num_layers=2\n",
    "        self.lstm = nn.LSTM(input_size=2, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Classification head for each digit position\n",
    "        self.fc = nn.Linear(hidden_size, 10)  # Output size is 10 (for digits 0-9)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Initializes the hidden state and cell state for the LSTM dynamically based on LSTM configuration.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): The size of the batch.\n",
    "            device (torch.device): The device (CPU or GPU).\n",
    "        \n",
    "        Returns:\n",
    "            tuple: The initial hidden state and cell state for the LSTM.\n",
    "        \"\"\"\n",
    "        # Dynamically get the number of layers and hidden size from the LSTM configuration\n",
    "        num_layers = self.lstm.num_layers\n",
    "        hidden_size = self.lstm.hidden_size\n",
    "\n",
    "        # LSTM requires both hidden and cell state for each layer\n",
    "        hidden = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        cell = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, seq1, seq2):\n",
    "        batch_size = seq1.size(0)  # Get the batch size\n",
    "        device = seq1.device  # Get the device (CPU or GPU)\n",
    "\n",
    "        # Initialize hidden state (and cell state) for the LSTM at the start of the batch\n",
    "        hidden = self.init_hidden(batch_size, device)\n",
    "\n",
    "        # Reverse the sequences (from right to left)\n",
    "        seq1 = torch.flip(seq1, dims=[1])  # Reverse seq1 along the sequence length dimension\n",
    "        seq2 = torch.flip(seq2, dims=[1])  # Reverse seq2 along the sequence length dimension\n",
    "        \n",
    "        # Combine the two sequences by stacking them along a new dimension\n",
    "        x = torch.stack((seq1, seq2), dim=2).float()  # Shape: [batch_size, seq_len, 2] (input_size=2)\n",
    "        \n",
    "        # Run the sequences through the LSTM\n",
    "        rnn_out, (hidden, cell) = self.lstm(x, hidden)\n",
    "\n",
    "        # Reverse the output to match the original left-to-right sequence\n",
    "        rnn_out = torch.flip(rnn_out, dims=[1])  # Reverse rnn_out along the sequence length dimension\n",
    "        \n",
    "        # Apply the classification head to each time step\n",
    "        output = self.fc(rnn_out)  # Shape: [batch_size, seq_len, 10]\n",
    "        \n",
    "        # Permute the output to [batch_size, 10, seq_len] for CrossEntropyLoss\n",
    "        return output.permute(0, 2, 1)  # Shape: [batch_size, 10, seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:36<00:00, 13.80it/s, loss=0.00221]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_size = 128  # Hidden size of the RNN\n",
    "\n",
    "\n",
    "rnn_addition_model = RNNAddition(hidden_size=hidden_size, num_layers=2) \n",
    "\n",
    "# Assuming train_loader is already created with the DigitAdditionDataset\n",
    "train_addition(rnn_addition_model, train_loader, num_epochs=500, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Summed Sequences: 500/500, Accuracy: 100.00%\n",
      "Number of Correctly Summed Sequences: 754/1000, Accuracy: 75.40%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the RNN model on the test dataset\n",
    "acc_trn_rnn_addition = evaluate_addition(rnn_addition_model, train_loader)\n",
    "acc_tst_rnn_addition = evaluate_addition(rnn_addition_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "A **language model (LM)** is a type of model that learns to predict the next token in a sequence, given a preceding context. An example of an LM, duh, ChatGPT! So, let's design one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mysterious Island\n",
    "\n",
    "**Jules Verne** (1828â€“1905) was a French novelist, and poet, widely regarded as one of the pioneers of science fiction. His adventure novels, filled with detailed scientific explanations. Some of his most famous works include **\"Twenty Thousand Leagues Under the Sea\"**, **\"Around the World in Eighty Days\"**, and **\"Journey to the Center of the Earth\"**.\n",
    "\n",
    "**\"The Mysterious Island\"** is the story of five castaways who find themselves stranded on a mysterious, uninhabited island. Using their ingenuity and resourcefulness, they attempt to survive, all while uncovering the secrets of the island. In this task, weâ€™ll use **\"The Mysterious Island\"** as the training data to build an **LM**, maybe we can mimic Verneâ€™s unique writing style.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"data/Jules_Verne.jpg\" alt=\"Jules Verne\" style=\"width: 25%;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1131713\n",
      "Unique Characters: 88\n"
     ]
    }
   ],
   "source": [
    "## Reading and processing text\n",
    "with open(\"data/The_Mysterious_Island.txt\", \"r\", encoding=\"utf8\") as jv_file:\n",
    "    jv_book_text = jv_file.read()\n",
    "\n",
    "# Create a set of unique characters\n",
    "jv_char_set = set(jv_book_text)\n",
    "\n",
    "# Print results\n",
    "print('Total Length:', len(jv_book_text))\n",
    "print('Unique Characters:', len(jv_char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "A tokenizer processes text and converts it to a sequence of symbols or tokens that can be fed into a model. In this task, weâ€™ll use a simple tokenizer that converts the text to lowercase and splits it into words. The **Tokenizer** class is responsible for:\n",
    "1. **Creating a Vocabulary**: The tokenizer identifies all unique characters in the input text and builds a vocabulary, which is a sorted list of these unique characters.\n",
    "2. **Character-to-Integer Mapping**: Once the vocabulary is established, the tokenizer creates a dictionary that maps each character to a unique integer index (`char2int`). This enables the model to work with numbers rather than raw text.\n",
    "3. **Integer-to-Character Mapping**: The tokenizer also builds the reverse mapping (`int2char`), which allows the model to convert predicted integers back into characters for generating readable text.\n",
    "\n",
    "\n",
    "- `encode(text)`: This method converts input text into a list of integer values based on the character-to-integer mapping. The output is a NumPy array of integers, where each integer represents a character in the text.\n",
    "  \n",
    "  Example:\n",
    "  ```python\n",
    "  tokenizer.encode('Hello') \n",
    "  # Output: [8, 4, 11, 11, 14]  (based on the vocabulary and char2int mapping)\n",
    "  ```\n",
    "\n",
    "- `decode(encoded_text)`: This method converts a list of integers (representing characters) back into the original text using the integer-to-character mapping.\n",
    "  \n",
    "  Example:\n",
    "  ```python\n",
    "  tokenizer.decode([8, 4, 11, 11, 14]) \n",
    "  # Output: 'Hello'  (converts integers back to the corresponding characters)\n",
    "  ```\n",
    "\n",
    "\n",
    "BTW, check this out [OpenAI Tokenizer](https://platform.openai.com/tokenizer)! ðŸ¤–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text):\n",
    "        # Create the vocabulary (sorted set of unique characters)\n",
    "        self.vocab = sorted(set(text))\n",
    "        self.char2int = {ch: i for i, ch in enumerate(self.vocab)}\n",
    "        self.int2char = np.array(self.vocab)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Converts text into an array of integers.\"\"\"\n",
    "        return np.array([self.char2int[ch] for ch in text], dtype=np.int32)\n",
    "    \n",
    "    def decode(self, encoded_text):\n",
    "        \"\"\"Converts an array of integers back into text.\"\"\"\n",
    "        return ''.join(self.int2char[encoded_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #3. Study the character-level tokenizer</span>\n",
    "\n",
    "To have a short and fun break, study the `Tokenizer` class provided below. Understand how it works and how it can be used to encode and decode text. Feel free to experiment with the tokenizer by encoding and decoding different texts.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: hello ece4179\n",
      "Encoded tokens: [64 61 68 68 71  1 61 59 61 18 15 21 23]\n",
      "Decoded prompt from tokens: hello ece4179\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Tokenizer with the text\n",
    "jv_tokenizer = Tokenizer(jv_book_text)\n",
    "\n",
    "# Provide a custom text for encoding and decoding\n",
    "prompt = \"hello ece4179\"\n",
    "\n",
    "# Encode the custom text\n",
    "enc_tokens = jv_tokenizer.encode(prompt)\n",
    "\n",
    "# Print the encoded custom text\n",
    "print('Prompt:', prompt)\n",
    "print('Encoded tokens:', enc_tokens)\n",
    "\n",
    "# Decode the encoded custom text back to the original text\n",
    "dec_prompt = jv_tokenizer.decode(enc_tokens)\n",
    "\n",
    "# Print the decoded text\n",
    "print('Decoded prompt from tokens:', dec_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "\n",
    "The **TextDataset** class is responsible for preparing the training data for our LM. It encodes the input text into integer sequences and creates pairs of input-target data that the model uses to learn how to predict the next character in a sequence.\n",
    "\n",
    "\n",
    "When training a character-level LM, the goal is to feed sequences of characters (input) into the model and have it predict the next character (target). The **TextDataset** class constructs the training data by creating sliding windows of sequences from the encoded text.\n",
    "\n",
    "The main idea is to slide a window of `seq_length` characters across the encoded text. For each index idx, the dataset generates an input sequence and its corresponding target sequence. The input sequence is a chunk of seq_length characters starting at position idx in the encoded text. The target sequence is created by shifting the input sequence by one character. This teaches the model to predict the next character for each position in the input sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, book_text, tokenizer, seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        # Encode the entire book text using the tokenizer\n",
    "        self.text_encoded = self.tokenizer.encode(book_text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # The number of available chunks in the dataset\n",
    "        return len(self.text_encoded) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate input sequence and target dynamically\n",
    "        input_seq = self.text_encoded[idx:idx + self.seq_length]\n",
    "        target_char = self.text_encoded[idx+1:idx + self.seq_length+1]\n",
    "        return torch.tensor(input_seq).long(), torch.tensor(target_char).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: tensor([87, 48, 64, 61,  1, 44, 74, 71, 66, 61, 59, 76,  1, 35, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 33, 30, 71, 71, 67,  1, 71, 62,  1, 48, 64, 61,  1,\n",
      "        41, 81, 75, 76])\n",
      "Target character: tensor([48., 64., 61.,  1., 44., 74., 71., 66., 61., 59., 76.,  1., 35., 77.,\n",
      "        76., 61., 70., 58., 61., 74., 63.,  1., 33., 30., 71., 71., 67.,  1.,\n",
      "        71., 62.,  1., 48., 64., 61.,  1., 41., 81., 75., 76., 61.])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 40  # Sequence length for training\n",
    "jv_dataset = TextDataset(jv_book_text, jv_tokenizer, seq_length=seq_length)\n",
    "\n",
    "batch_size = 64\n",
    "jv_dataloader = DataLoader(jv_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# let's print the first sample from the dataset\n",
    "trn_x_sample, trn_y_sample = jv_dataset[0]\n",
    "print(f'Input sequence: {trn_x_sample}')\n",
    "print(f'Target character: {trn_y_sample}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2b0080; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### <span style=\"color: pink;\">Task #4. Create an RNN for Character-Level LM</span>\n",
    "\n",
    "Your RNN should consist of the following components:\n",
    "\n",
    "1. **Embedding Layer**: This layer will map each character (represented as an integer) into a dense vector of fixed size (`embed_dim`). The embedding allows the model to learn meaningful representations of each character.\n",
    "\n",
    "2. **LSTM Layer**: The core of your RNN model will be an **LSTM** (Long Short-Term Memory) layer. This layer will process the input sequences and capture patterns over time.\n",
    "\n",
    "3. **Fully Connected (Linear) Layer**: After processing the sequence with the LSTM, a fully connected layer will map the hidden states back to the vocabulary space, allowing the model to predict the next character.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class jv_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden.to(device), cell.to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jv_RNN(\n",
       "  (embedding): Embedding(88, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=88, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(jv_tokenizer.vocab)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "# instantiate the model\n",
    "jv_model = jv_RNN(vocab_size, embed_dim, rnn_hidden_size) \n",
    "jv_model = jv_model.to(device)\n",
    "jv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop for LM\n",
    "\n",
    "The training loop for the LM is similar to the training loop for the previous task to a great degree. The loss function used in the training loop is the **CrossEntropyLoss**. We will train the model for fixed number of iterations in the workshop, feel free to improve the model by training it for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [11:34<00:00, 14.41it/s, loss=1.03] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(jv_model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 10000\n",
    "print_every_k = 100  # Print and update the progress bar every `k` iterations\n",
    "batch_size = 64  # Example batch size\n",
    "\n",
    "\n",
    "# Initialize the progress bar for iterations\n",
    "pbar = tqdm(total=num_iterations, desc=\"Training Progress\", position=0, leave=True)\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "\n",
    "    seq_batch, target_batch = next(iter(jv_dataloader))\n",
    "    \n",
    "    # Initialize hidden and cell states\n",
    "    hidden, cell = jv_model.init_hidden(batch_size)\n",
    "\n",
    "    # Move batch to the correct device (CPU/GPU)\n",
    "    seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass through the model with the entire sequence\n",
    "    pred, hidden, cell = jv_model(seq_batch, hidden, cell)\n",
    "    pred = pred.view(-1, vocab_size)  # Reshape to [batch_size,  seq_length, vocab_size]\n",
    "    # Compute loss for the last time step prediction\n",
    "    loss = loss_fn(pred, target_batch.view(-1).long())\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Accumulate total loss for this iteration\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Update the progress bar and print every `k` iterations\n",
    "    if iteration % print_every_k == 0:\n",
    "        # Calculate the average loss over the last `k` iterations\n",
    "        avg_loss = total_loss / print_every_k\n",
    "        \n",
    "        # Print and update tqdm\n",
    "        # print(f\"Iteration {iteration}/{num_iterations}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Update the progress bar with the average loss\n",
    "        pbar.set_postfix(loss=avg_loss)\n",
    "        pbar.update(print_every_k)  # Update the progress bar by 'k' steps\n",
    "        \n",
    "        # Reset total_loss for the next `k` iterations\n",
    "        total_loss = 0\n",
    "\n",
    "# Close the progress bar when training is done\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write with your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def sample(model, starting_str, len_generated_text=500, temperature=1.0):\n",
    "    # Encode the starting string\n",
    "    encoded_input = jv_tokenizer.encode(starting_str)\n",
    "    encoded_input = torch.tensor(encoded_input).unsqueeze(0)  # Shape [1, seq_length]\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)  # For batch size of 1\n",
    "    hidden, cell = hidden.to('cpu'), cell.to('cpu')\n",
    "\n",
    "    # Pass the entire starting string through the model to prime the hidden state\n",
    "    _, hidden, cell = model(encoded_input, hidden, cell)\n",
    "\n",
    "    # Start sampling from the last character of the input\n",
    "    last_char = encoded_input[:, -1]  # Get the last character of the input\n",
    "\n",
    "    # Generate the next characters\n",
    "    for i in range(len_generated_text):\n",
    "        # Forward pass for the last character\n",
    "        logits, hidden, cell = model(last_char.view(1,1), hidden, cell)  # Shape [1, 1]\n",
    "        logits = torch.squeeze(logits, 0)  # Remove batch dimension\n",
    "\n",
    "        # Scale logits by temperature (divide by temperature)\n",
    "        scaled_logits = logits / temperature\n",
    "\n",
    "        # Sample from the scaled distribution\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "\n",
    "        # Decode and append the generated character to the string\n",
    "        generated_str += jv_tokenizer.decode([last_char.item()])\n",
    "\n",
    "    return generated_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The islanders of the water would have been contained on the shore of the Mercy. The colonists was difficult to do the return of this species of a few days they had taken principal of the island.\n",
      "\n",
      "It was not a straight and southern point of the island, and the colonists then to see the convicts could not be used in a corner of a fire which we will not be surprised by the convicts and the coast, and the two companions but the engineer had not been able to arrest to the forest. The poor of the island was not\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jv_model.to('cpu')\n",
    "print(sample(jv_model, starting_str='The island', len_generated_text=500, temperature=0.50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE4179",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
